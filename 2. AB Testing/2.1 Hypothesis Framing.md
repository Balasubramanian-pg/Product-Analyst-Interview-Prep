### **Hypothesis Framing: How to Structure Data-Driven Experiments**
A well-framed hypothesis turns vague ideas into testable predictions, ensuring your experiments (like A/B tests) are focused and actionable. Here’s how to craft hypotheses that drive meaningful results:

### **What Makes a Strong Hypothesis?**
A good hypothesis is:
1. **Clear**: Specific about *what* you’re testing and *why*.
2. **Measurable**: Tied to a metric (e.g., conversion rate, retention).
3. **Actionable**: Directly informs a change you can implement.
4. **Falsifiable**: Can be proven true *or* false with data.

### **The Hypothesis Formula**
Use this template to structure your hypotheses:
> **"We believe [change] will cause [outcome] for [segment], because [rationale]."**

**Example**:
> *"We believe adding a progress bar to the signup flow will increase completion rates by 15% for new users, because it reduces perceived effort and clarifies steps."*

### **Breaking Down the Components**
| Component       | Questions to Answer                          | Example                                  |
|-----------------|---------------------------------------------|------------------------------------------|
| **Change**      | What are you modifying?                     | "Replacing the ‘Submit’ button with ‘Get Started’" |
| **Outcome**     | What metric will improve?                   | "Increase click-through rate by 20%"     |
| **Segment**     | Who is affected? (All users? New users?)    | "First-time mobile app users"            |
| **Rationale**   | Why do you expect this result?              | "‘Get Started’ is more action-oriented and aligns with user testing feedback." |

### **Types of Hypotheses**
1. **Descriptive**:
   - *"Users drop off most at Step 3 of checkout."* (Observation)
   - *Next step*: Hypothesize *why* (e.g., "The form is too long").

2. **Causal**:
   - *"Shortening the form to 3 fields will reduce abandonment by 10%."* (Testable prediction)

3. **Exploratory**:
   - *"Users who watch the tutorial video retain 30% better than those who skip it."* (Leads to further tests)

### **Example Hypotheses for Common Scenarios**
#### **1. Increasing Signups**
- *Weak*: "Let’s try a pop-up to get more signups."
- *Strong*:
  > "We believe a time-limited exit-intent popup (‘Sign up now for 20% off’) will increase email captures by 25% for desktop users, because urgency and discounts worked in past campaigns."

#### **2. Improving Feature Adoption**
- *Weak*: "Users aren’t using the new dashboard."
- *Strong*:
  > "We believe adding an in-app tutorial with a 3-step walkthrough will increase dashboard usage by 40% for users in their first week, because our survey showed 60% didn’t know it existed."

#### **3. Reducing Churn**
- *Weak*: "We need to email users more."
- *Strong*:
  > "We believe sending a personalized ‘We miss you’ email with a 10% discount to inactive users (no activity in 30 days) will reduce churn by 12%, because similar reactivation emails worked for Segment X in Q1."

### **How to Prioritize Hypotheses**
Use the **ICE Framework** to rank hypotheses by:
- **Impact**: How much will it move the needle? (High/Medium/Low)
- **Confidence**: How sure are you it’ll work? (Data, past tests, research)
- **Ease**: How hard is it to implement? (Low effort = quicker to test)



| Hypothesis                          | Impact | Confidence | Ease | ICE Score (I×C×E) |
|-------------------------------------|--------|------------|------|-------------------|
| Add progress bar to signup          | High   | High       | High | 27 (3×3×3)        |
| Change button color to red          | Low    | Medium     | High | 6 (1×2×3)         |
| Offer live chat during checkout     | High   | Low        | Low  | 3 (3×1×1)         |

*Focus on the highest ICE score first.*

### **Common Pitfalls**
1. **Vague predictions**: Avoid "This will improve engagement." → Specify *how much* and *for whom*.
2. **Ignoring "why"**: Always tie hypotheses to user behavior or data.
3. **Testing too broadly**: Narrow segments (e.g., "mobile users in India") for clearer insights.
4. **Confirming bias**: Don’t cherry-pick data—let results challenge assumptions.

### **Tools to Validate Hypotheses**
- **Quantitative**: Google Analytics, Mixpanel (track metrics).
- **Qualitative**: Hotjar (session recordings), UserTesting (feedback).
- **Experimentation**: Optimizely, VWO (A/B tests).

**For Balu PG**:
What’s a current challenge you’re facing? For example:
- **Low conversions** on a landing page?
- **Poor adoption** of a new feature?
- **High churn** after a free trial?

Share the context, and I’ll help you frame a hypothesis using the template above. We can also design an experiment to test it!

*Example*:
If you’re seeing **low engagement with a new chatbot**, a hypothesis might be:
> *"We believe moving the chatbot icon from the footer to the bottom-right corner will increase interactions by 30% for returning users, because heatmaps show users scroll less on mobile."*
