### **Statistical Significance in A/B Testing: What It Means and How to Use It**
Statistical significance tells you whether your A/B test results are **real** or due to random chance. Here’s how to interpret it correctly and avoid common traps:

### **Key Concepts**
1. **p-value**:
   - Probability that the observed difference is *not* due to your change (but random noise).
   - **p ≤ 0.05** (5%) is the standard threshold for "significance."
   - *Example*: p = 0.03 → 3% chance the result is a fluke.

2. **Confidence Level (1 – p)**:
   - How sure you are the result is real. 95% confidence = p ≤ 0.05.

3. **Confidence Interval (CI)**:
   - Range where the true effect likely lies.
   - *Example*: "Conversion rate improved by 12% ± 4%" (CI: 8–16%).

### **How to Interpret Significance**
| Scenario               | p-value | Conclusion                          | Action                          |
|------------------------|---------|-------------------------------------|---------------------------------|
| **Significant Winner** | p ≤ 0.05 | Variant B *likely* outperforms A.   | Roll out B.                     |
| **Not Significant**    | p > 0.05 | Inconclusive (could be chance).     | Keep testing or tweak the test. |
| **Borderline**         | 0.05 < p < 0.10 | Weak signal; needs more data.      | Extend the test.                |

### **Why 95% Confidence?**
- **5% false positive rate**: 1 in 20 "winning" tests could be wrong.
- **Trade-off**: Higher confidence (e.g., 99%) requires larger samples.

### **Common Mistakes**
1. **Peeking Early**:
   - Checking results mid-test inflates false positives. Set a fixed duration.
   - *Fix*: Use **sequential testing** (e.g., Bayesian methods) if you must monitor.

2. **Ignoring Practical Significance**:
   - A 0.1% lift might be "statistically significant" but irrelevant.
   - *Ask*: Is the effect *meaningful* for your business?

3. **Multiple Comparisons**:
   - Testing 10 variants? Even with p ≤ 0.05, ~1 "winner" could be random.
   - *Fix*: Use **Bonferroni correction** (divide p-value threshold by # of tests).

4. **Confusing Significance with Effect Size**:
   - A tiny lift (e.g., 0.5%) can be "significant" with huge samples but useless.
   - *Always check the CI*: Does it include 0? (e.g., CI: -1% to +3% → inconclusive).

### **Example: Significance in Action**


| Metric          | Control (A) | Variant (B) | p-value | 95% CI          | Decision               |
|-----------------|-------------|-------------|---------|-----------------|-------------------------|
| Conversion Rate | 10.0%       | 12.1%       | 0.02    | +1.2% to +3.0%  | **Significant winner**  |
| Revenue/User    | $45         | $46         | 0.15    | -$1 to +$3      | Not significant         |

*Takeaway*: Variant B lifts conversions, but revenue impact is unclear.

### **How to Calculate Significance**
1. **Use a calculator**:
   - Input conversions/visitors for A and B (e.g., [VWO’s tool](https://vwo.com/ab-test-significance-calculator/)).
2. **Check for overlap**:
   - If CIs for A and B overlap, the result is *not* significant.
   - *Example*:
     - A: 10% (CI: 8–12%)
     - B: 11% (CI: 9–13%)
     → **Overlap → No clear winner**.

3. **Bayesian Approach** (Alternative):
   - Reports "probability to beat baseline" (e.g., 98% chance B > A).
   - Tools: [Google’s Bayesian A/B Testing](https://github.com/google/sketchybayes).

### **When to Stop a Test**
- **Fixed Horizon**: Run for pre-calculated duration (e.g., 2 weeks).
- **Sequential Testing**: Stop when:
  - p ≤ 0.05 **and** sample size is met.
  - CI excludes 0 (for metrics like revenue).

### **What If Your Test Isn’t Significant?**
1. **Increase Sample Size**:
   - Extend the test or add more traffic.
2. **Check for Issues**:
   - Was the variant broken? (Use session recordings.)
   - Did external factors skew results? (e.g., a holiday sale).
3. **Replicate**:
   - Run a follow-up test to confirm.

**For Balu PG**:
Have you run a test recently? Share the:
- **Metric** (e.g., signups, revenue),
- **p-value** or **CI**, and
- **sample size per variant**.

I’ll help you interpret whether the result is trustworthy or needs more data.

*Example*:
If your test shows:
- **p = 0.07** (not significant) but **CI: +0.5% to +4.5%**,
we might conclude it’s *trending positive* but needs more users.

### **Pro Tip**
For low-traffic sites, consider:
- **Bandit Tests**: Dynamically allocate traffic to the better-performing variant.
- **Non-Parametric Tests**: Like the **Chi-square test** for small samples.
