### **Sample Size: How to Calculate It for Reliable A/B Tests**
The right sample size ensures your A/B test results are **statistically significant**—meaning you can trust the winner isn’t just luck. Here’s how to calculate it and avoid common mistakes:

### **Why Sample Size Matters**
- **Too small**: Results may be misleading (false positives/negatives).
- **Too large**: Wastes time/resources; delays decisions.
- **Just right**: Confidently detect meaningful differences.

### **Key Inputs for Sample Size Calculation**
1. **Baseline Conversion Rate**:
   - Current performance (e.g., 5% signup rate).
2. **Minimum Detectable Effect (MDE)**:
   - The smallest improvement you care about (e.g., a 10% lift to 5.5%).
3. **Statistical Power** (typically 80%):
   - Probability of detecting a true effect (avoids "false negatives").
4. **Significance Level (α)** (typically 5% or 0.05):
   - Risk of a false positive (e.g., 5% chance the "winner" is random).

### **Sample Size Formula (Simplified)**
For a two-tailed test (most A/B tests):
```
n = (16 * σ²) / Δ²
```
- **σ (sigma)**: Standard deviation (use ~0.5 for proportions like conversion rates).
- **Δ (Delta)**: Your MDE (e.g., 0.05 for a 5% lift).

**Example**:
- Baseline: 10% conversion.
- MDE: 2% lift (to 12%).
- Power: 80%, Significance: 95%.
→ **~4,000 users per variant** (8,000 total).

*(Use a calculator like [Optimizely’s](https://www.optimizely.com/sample-size-calculator/) or [VWO’s](https://vwo.com/ab-test-significance-calculator/) for precision.)*

### **Sample Size Table for Common Scenarios**


| Baseline Rate | MDE (Lift) | Sample Size/Variant |
|---------------|------------|---------------------|
| 5%            | 1% (→6%)   | ~25,000             |
| 10%           | 2% (→12%)  | ~4,000              |
| 20%           | 3% (→23%)  | ~1,500              |
| 50%           | 5% (→55%)  | ~700                |

*Note*: Higher baseline rates or larger MDEs require fewer users.

### **How to Adjust for Your Test**
1. **Segmentation**:
   - Testing only mobile users? Ensure your sample reflects that traffic.
2. **Test Duration**:
   - Divide sample size by daily visitors to estimate runtime.
   - *Example*: 4,000 users/variant ÷ 500 daily visitors = **8 days**.
3. **Unequal Split**:
   - Using 70/30 traffic allocation? Adjust sample sizes proportionally.

### **Common Mistakes**
1. **Stopping Too Early**:
   - Peeking at results mid-test inflates false positives. Set a fixed duration.
2. **Ignoring Variance**:
   - High-variance metrics (e.g., revenue per user) need larger samples.
3. **Forgetting Seasonality**:
   - Run tests over full weekly cycles (e.g., avoid starting on a Friday).

### **Tools to Calculate Sample Size**
| Tool                          | Best For                     | Link                                  |
|-------------------------------|------------------------------|---------------------------------------|
| Optimizely Sample Size Calculator | Quick estimates              | [Link](https://www.optimizely.com/sample-size-calculator/) |
| VWO A/B Test Calculator       | Advanced (confidence intervals) | [Link](https://vwo.com/ab-test-significance-calculator/) |
| Evan’s Awesome A/B Tools      | Bayesian methods             | [Link](https://www.evanmiller.org/ab-testing/sample-size.html) |

### **What If You Can’t Hit the Ideal Sample Size?**
- **Prioritize high-impact tests** (use ICE scoring).
- **Run sequential tests** (e.g., bandit algorithms for dynamic allocation).
- **Accept higher MDEs** (e.g., only detect 5%+ lifts instead of 1%).

**For Balu PG**:
What’s the **baseline metric** you’re testing (e.g., conversion rate, click-through rate) and the **minimum lift** you want to detect?
For example:
- *"Our checkout completion rate is 15%, and we want to detect a 3% improvement."*
→ I can calculate the exact sample size and duration needed.

*Pro Tip*: If your traffic is low, consider:
- **Multi-armed bandit tests** (gradually shift traffic to the winner).
- **Testing bigger changes** (larger MDEs = smaller samples).
