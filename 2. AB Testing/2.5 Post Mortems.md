### **Post-Mortems: How to Learn from Failures (and Successes)**
A **post-mortem** (or "retrospective") is a structured analysis of what went wrong (or right) in a project, experiment, or incident. Its goal is to **extract actionable lessons**—not to assign blame. Here’s how to run one effectively:


### **When to Conduct a Post-Mortem**
| Scenario                     | Example                          | Focus Areas                          |
|------------------------------|----------------------------------|--------------------------------------|
| **Failed A/B Test**          | New checkout flow hurt conversions | Hypothesis, execution, data quality |
| **Product Launch Flop**      | Feature adoption was 10% of goal  | User research, onboarding, messaging |
| **System Outage**            | App crashed during peak traffic   | Root cause, monitoring, response time|
| **Successful Experiment**    | Campaign exceeded KPIs by 30%     | What worked? How to replicate?      |

### **Post-Mortem Template**
Use this structure to keep discussions **focused and actionable**:

#### **1. Background**
- What happened? (Brief timeline)
- What was the expected vs. actual outcome?
- *Example*:
  > *"We launched a new referral program on May 1, expecting 5,000 signups in Week 1. Only 800 users participated."*

#### **2. Impact**
- Quantitative: Metrics affected (e.g., revenue, user drop-off).
- Qualitative: User feedback, team effort wasted.
- *Example*:
  > *"Referral signups: -84% vs. goal. Support tickets spiked by 200% due to confusion about rewards."*

#### **3. Root Causes** (Use the **5 Whys** Technique)
Ask "why?" repeatedly to dig deeper.
- *Example*:
  1. Why did signups drop? → Users didn’t understand the referral link.
  2. Why? → The instructions were hidden in a dropdown.
  3. Why? → We assumed users would explore the FAQ.
  4. Why? → No user testing was done pre-launch.
  5. **Root cause**: Skipped UX validation.

#### **4. Lessons Learned**
- What **processes** failed? (e.g., lack of user testing)
- What **assumptions** were wrong? (e.g., "Users will find the FAQ")
- *Example*:
  > *"Assumed clarity ≠ tested clarity. Always validate onboarding with 5+ users pre-launch."*

#### **5. Action Items**
- **Immediate fixes**: Patch the issue (e.g., move referral instructions to the homepage).
- **Process changes**: Prevent recurrence (e.g., mandate user testing for new features).
- **Owners + Deadlines**: Assign responsibility.
- *Example*:
  | Action Item                          | Owner       | Deadline   |
  |--------------------------------------|-------------|------------|
  | Add referral CTA to homepage         | Design Team | May 15     |
  | Require UX testing for all launches  | PM          | May 30     |

### **How to Run a Blameless Post-Mortem**
1. **Focus on systems, not people**:
   - *"The process failed"* vs. *"You failed."*
2. **Use data, not opinions**:
   - *"Analytics showed 60% dropped off at Step 2"* vs. *"I think the flow was confusing."*
3. **Encourage honesty**:
   - Frame it as a **learning opportunity**, not a witch hunt.
4. **Document everything**:
   - Share findings with the team (e.g., Confluence, Notion, or a shared doc).

### **Post-Mortem for A/B Tests**
If your experiment failed, ask:
1. **Hypothesis**:
   - Was it based on data or a guess?
   - *Example*: *"We assumed a red button would convert better, but no user research supported this."*
2. **Execution**:
   - Did the test run long enough? Were segments mixed?
   - *Example*: *"Test ran during a holiday sale, skewing results."*
3. **Analysis**:
   - Was the data clean? Did you check for significance correctly?
   - *Example*: *"We ignored that mobile users saw a broken variant."*
4. **External Factors**:
   - Did a competitor launch something? Was there a bug?
   - *Example*: *"A simultaneous email campaign drove traffic to the old flow."*

### **Example: Post-Mortem for a Failed Feature Launch**
**Background**:
Launched a new "Save for Later" cart feature on April 1. Expected 30% usage; only 5% used it.

**Root Causes**:
1. **Poor discovery**: Button was gray and blended into the background.
2. **No incentive**: Users didn’t see the value (no email reminders).
3. **Technical issue**: Button didn’t work on Safari (20% of users).

**Action Items**:
| Fix                              | Owner          | Deadline   |
|----------------------------------|----------------|------------|
| Change button to blue + add tooltip | Design        | April 15   |
| Add email reminder for saved items | Marketing     | April 20   |
| Fix Safari bug                   | Engineering   | April 10   |
| Require QA testing on all browsers | QA Lead       | Ongoing    |

### **Post-Mortem for Successes**
Even wins deserve scrutiny! Ask:
- **Why did this work?** (e.g., Was it the messaging, timing, or audience?)
- **Can we replicate it?** (e.g., Apply the same tactic to other campaigns.)
- **What risks did we overlook?** (e.g., Did it cannibalize another metric?)

*Example*:
> *"Our Black Friday campaign exceeded goals by 40%. Post-mortem revealed it worked because we:
> - Targeted past purchasers (not cold traffic).
> - Used urgency (‘24-hour deal’) in emails.
> *Action*: Apply this segmentation to all future promotions."*

**For Balu PG**:
What’s a recent **test, launch, or incident** you’d like to analyze? For example:
- A **failed A/B test** (e.g., a new pricing page underperformed)?
- A **product feature** that flopped or exceeded expectations?
- A **technical outage** (e.g., app crashes during a sale)?

Share the context, and I’ll help you:
1. Structure the post-mortem.
2. Identify root causes.
3. Draft actionable next steps.

*Pro Tip*: If the issue was **low feature adoption**, we might dig into:
- Was the hypothesis validated with user research?
- Did the onboarding flow fail?
- Were there technical barriers (e.g., slow load times)?
